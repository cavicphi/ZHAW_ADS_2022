{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRTa3Ee15WsJ"
   },
   "source": [
    "# ADS Project 2022 Gruppe 7:  Image Classification with Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stuff for web scraping\n",
    "import mechanicalsoup\n",
    "import wget\n",
    "\n",
    "#stuff to handle files and folders\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "#other stuff\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to kaggle api\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download dataset from kaggle using api\n",
    "api.dataset_download_files('shaunthesheep/microsoft-catsvsdogs-dataset', path='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzip the zip file\n",
    "import zipfile\n",
    "with zipfile.ZipFile('microsoft-catsvsdogs-dataset.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('catsvsdogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize a subset of images of dogs\n",
    "dst_dir = 'catsvsdogs/resized/dogs'\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "files = glob.glob('./catsvsdogs/PetImages/Dog/*.jpg')\n",
    "\n",
    "i = 0\n",
    "\n",
    "for f in files: \n",
    "    i = i + 1\n",
    "    try:\n",
    "        img = Image.open(f)\n",
    "    except (IOError) as e: #in some instances there were some corrupted files in the zip file.\n",
    "        print ('Bad file:', f)\n",
    "        pass\n",
    "    img = Image.open(f)\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    img_resize = img.resize((192, 192))\n",
    "    root, ext = os.path.splitext(f)\n",
    "    basename = os.path.basename(root)\n",
    "    img_resize.save(os.path.join(dst_dir, basename + ext))\n",
    "    if i == 3000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resize a subset of images of cats \n",
    "dst_dir = 'catsvsdogs/resized/cats'\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "files = glob.glob('./catsvsdogs/PetImages/Cat/*.jpg')\n",
    "\n",
    "i = 0\n",
    "\n",
    "for f in files:\n",
    "    i = i + 1\n",
    "    try:\n",
    "        img = Image.open(f)\n",
    "    except (IOError) as e: #in some instances there were some corrupted files in the zip file.\n",
    "        print ('Bad file:', f)\n",
    "        pass\n",
    "    if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "    img_resize = img.resize((192, 192))\n",
    "    root, ext = os.path.splitext(f)\n",
    "    basename = os.path.basename(root)\n",
    "    img_resize.save(os.path.join(dst_dir, basename + ext))\n",
    "    if i == 3000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making sure the folders were created correctly\n",
    "data_dir = os.path.join(os.curdir, 'catsvsdogs/resized')\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONAL: Delete original unziped of data\n",
    "\n",
    "shutil.rmtree(os.path.join(os.curdir, 'catsvsdogs/PetImages'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GoKGm1duzgk"
   },
   "source": [
    "## Image Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ro4oYaEmxe4r",
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (192, 192)\n",
    "\n",
    "#shuffle immer mit seed, sonst kann man nicht das gleiche Dataset in Valdiation & Train verwenden.\n",
    "\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(data_dir,\n",
    "                                                            batch_size=BATCH_SIZE,\n",
    "                                                            image_size=IMG_SIZE,\n",
    "                                                            shuffle=True,\n",
    "                                                            seed=999,\n",
    "                                                            validation_split=0.20,\n",
    "                                                            subset = \"training\")\n",
    "\n",
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(data_dir,\n",
    "                                                            batch_size=BATCH_SIZE,\n",
    "                                                            image_size=IMG_SIZE,\n",
    "                                                            shuffle=True,\n",
    "                                                            seed=999,\n",
    "                                                            validation_split=0.20,\n",
    "                                                            subset = \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5BeQyKThC_Y",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_names = train_dataset.class_names\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_dataset.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZqCX_mpV3Mx"
   },
   "source": [
    "### Creation of test set by splitting the validation set into batches and keep 1 for the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFFIYrTFV9RO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_batches = tf.data.experimental.cardinality(validation_dataset)\n",
    "test_dataset = validation_dataset.take(val_batches // 10)\n",
    "val_dataset = validation_dataset.skip(val_batches // 10)\n",
    "\n",
    "#OPEN QUESTION\n",
    "\n",
    "#Das Abändern der grünen Zahl beeinflusst die Accuracy stark. Warum?\n",
    "#Was geschieht hier wirklich?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q9pFlFWgBKgH",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\n",
    "print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22XWC7yjkZu4"
   },
   "source": [
    "It is convenient to use buffered prefetching to load images from disk without having I/O become blocking. That is what the next cell does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3UUPdm86LNC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "#OPEN QUESTION\n",
    "\n",
    "#Was geschieht hier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYfcVwYLiR98"
   },
   "source": [
    "### Use data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3P99QiMGit1A",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#To reduce overfitting, pictures are flipped and rotated\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "      tf.keras.layers.RandomFlip('horizontal'),\n",
    "      tf.keras.layers.RandomRotation(0.2),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mD3rE2Lm7-d"
   },
   "source": [
    "Let's repeatedly apply these layers to the same image and see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQullOUHkm67",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test effect of augmentation\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    for image, _ in train_dataset.take(1):\n",
    "      plt.figure(figsize=(10, 10))\n",
    "      first_image = image[0]\n",
    "      for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
    "        plt.imshow(augmented_image[0] / 255)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAywKtuVn8uK"
   },
   "source": [
    "### Choose Transfer Learning Model \n",
    "\n",
    "As a first execution of the notebook, we will use the following pre-trained model: **MobileNetV2** developed by Google (dowloaded via  `tf.keras.applications.MobileNetV2` ). This will be your base model. The big advantage is that it was trained on ImageNet data! So 1.4M images :) with a wide variety of categories like `jackfruit` and `syringe`. Not on smiles though...Will this help us in recognizing smiles??\n",
    "\n",
    "Note: Every model was trained using specific image preprocessing requirements. YOU NEED TO CHECK EACH TIME WHAT PECULIAR OF A SPECIFIC MODEL!\n",
    "Here the guide for MobileNetV2 https://keras.io/api/applications/mobilenet/.\n",
    "\n",
    "This model expects pixel values in `[-1, 1]`, but at this point, the pixel values in your images are in `[0, 255]`. \n",
    "At best, you always use the preprocessing method included with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cO0HM9JAQUFq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2NyJn4KQMux",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#rescale = tf.keras.layers.Rescaling(1./127.5, offset=-1)\n",
    "\n",
    "#OPEN QUESTION\n",
    "\n",
    "#Wäre das noch nötig?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkH-kazQecHB"
   },
   "source": [
    "We saw in the lesson that, wen doing trasnfer learning, we don't use all the architecture but only the feature extraction.\n",
    "The very last classification layer (on \"top\", as most diagrams of machine learning models go from bottom to top) is not very useful. Instead, you will follow the common practice to depend on the very last layer before the flatten operation. This layer is called the \"bottleneck layer\". The bottleneck layer features retain more generality as compared to the final/top layer.\n",
    "\n",
    "First, instantiate a MobileNet V2 model pre-loaded with weights trained on ImageNet. By specifying the **include_top=False** argument, you load a network that doesn't include the classification layers at the top, which is ideal for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19IQ2gqneqmS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqcsxoJIEVXZ"
   },
   "source": [
    "This feature extractor converts each `192x192x3` image into a `6x6x1280` block of features. Let's see what it does to an example batch of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-2LJL0EEUcx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(train_dataset))\n",
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlx56nQtfe8Y"
   },
   "source": [
    "### a. Freeze weights, use the feature extraction as it is, attach a new classification head\n",
    "In this step, you will freeze the convolutional base created from the previous step and to use as a feature extractor. Additionally, you add a classifier on top of it and train the top-level classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fL6upiN3ekS"
   },
   "source": [
    "It is important to freeze the convolutional base before you compile and train the model. Freezing (by setting layer.trainable = False) prevents the weights in a given layer from being updated during training. MobileNet V2 has many layers, so setting the entire model's `trainable` flag to False will freeze all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTCJH4bphOeo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#freeze convolutional base before compiling and training the model.\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the base model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KpbzSmPkDa-N",
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now combine this model with a classification head, meaning some fully connected layers (tf.keras.layers.Dense). \n",
    "\n",
    "Build a model by chaining together the data augmentation, preproccesing, `base_model` and classification layers (note that the syntax is slighly different, we are using the [Keras Functional API]).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1p0OJBR6dOT"
   },
   "source": [
    "Apply a `tf.keras.layers.Dense` layer to convert these features into a single prediction per image. You don't need an activation function here because this prediction will be treated as a `logit`, or a raw prediction value. Positive numbers predict class 1, negative numbers predict class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DgzQX6Veb2WT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False) #We need to set `training=False` as our model contains a `BatchNormalization` layer. More explanation here https://www.tensorflow.org/guide/keras/transfer_learning\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x) #this layer has the same purpose as tf.keras.layers.Flatten(). Need to connect something 2D to something 1D. Ask if you want to know more :)\n",
    "#x = tf.keras.layers.Dense(10, activation=\"relu\")(x)\n",
    "#x = tf.keras.layers.Dropout(0.2)(x)\n",
    "#x = tf.keras.layers.Dense(5, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x) \n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "#OPEN QUESTION\n",
    "\n",
    "#Given the great performance, are more layers necessary? Didn't try what effect it would have yet...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0ylJXE_kRLi"
   },
   "source": [
    "Compile the model before training it. Since there are two classes, use the `tf.keras.losses.BinaryCrossentropy` loss with `from_logits=True` since the model provides a linear output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpR8HdyMhukJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I8ARiyMFsgbH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxOcmVr0ydFZ"
   },
   "source": [
    "Above you see how many parameters you are actually training, and how many are frozen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxvgOYTDSWTx"
   },
   "source": [
    "### Train the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Om4O3EESkab1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_epochs = 10\n",
    "\n",
    "loss0, accuracy0 = model.evaluate(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cYT1c48CuSd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"initial loss: {:.2f}\".format(loss0))\n",
    "print(\"initial accuracy: {:.2f}\".format(accuracy0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsaRFlZ9B6WK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hd94CKImf8vi"
   },
   "source": [
    "\n",
    "Let's take a look at the learning curves of the training and validation accuracy/loss when using the MobileNetV2 base model as a fixed feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53OTCh3jnbwV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plot learning curves of training and validation accuracy / loss.\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foWMyyUHbc1j"
   },
   "source": [
    "Note: If you are wondering why the validation metrics are better than the training metrics, the main factor is because layers like `tf.keras.layers.BatchNormalization` and `tf.keras.layers.Dropout` affect accuracy during training. They are turned off when calculating validation loss.\n",
    "\n",
    "To a lesser extent, it is also because training metrics report the average for an epoch, while validation metrics are evaluated after the epoch, so validation metrics see a model that has trained slightly longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqwV-CRdS6Nv"
   },
   "source": [
    "### b) Fine tuning - Überhaupt notwendig bei so hoher Accuracy?\n",
    "In the feature extraction experiment, you were only training a few layers on top of an MobileNetV2 base model. The weights of the pre-trained network were **not** updated during training.\n",
    "\n",
    "One way to increase performance even further is to train (or \"fine-tune\") the weights of the top layers of the pre-trained model alongside the training of the classifier you added. The training process will force the weights to be tuned from generic feature maps to features associated specifically with the dataset.\n",
    "\n",
    "Note: This should only be attempted after you have trained the top-level classifier with the pre-trained model set to non-trainable. If you add a randomly initialized classifier on top of a pre-trained model and attempt to train all layers jointly, the magnitude of the gradient updates will be too large (due to the random weights from the classifier) and your pre-trained model will forget what it has learned.\n",
    "\n",
    "Also, you should try to fine-tune a small number of top layers rather than the whole MobileNet model. In most convolutional networks, the higher up a layer is, the more specialized it is. The first few layers learn very simple and generic features that generalize to almost all types of images. As you go higher up, the features are increasingly more specific to the dataset on which the model was trained. The goal of fine-tuning is to adapt these specialized features to work with the new dataset, rather than overwrite the generic learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfxv_ifotQak"
   },
   "source": [
    "All you need to do is unfreeze the `base_model` and set the bottom layers to be un-trainable. Then, you should recompile the model (necessary for these changes to take effect), and resume training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nzcagVitLQm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4HgVAacRs5v",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's take a look to see how many layers are in the base model\n",
    "#print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "#fine_tune_at = 100\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "#for layer in base_model.layers[:fine_tune_at]:\n",
    "#  layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Uk1dgsxT0IS"
   },
   "source": [
    "As you are training a much larger model and want to readapt the pretrained weights, it is important to use a lower learning rate at this stage. Otherwise, your model could overfit very quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NtUnaz0WUDva",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "#              optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate/10),\n",
    "#              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwBWy7J2kZvA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0foWUN-yDLo_"
   },
   "source": [
    "Let's keep training from where we stopped before, so we can keep improving hopefully. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECQLkAsFTlun",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fine_tune_epochs = 10\n",
    "#total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "#history_fine = model.fit(train_dataset,\n",
    "#                         epochs=total_epochs,\n",
    "#                         initial_epoch=history.epoch[-1],\n",
    "#                         validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfXEmsxQf6eP"
   },
   "source": [
    "Let's take a look at the learning curves of the training and validation accuracy/loss when fine-tuning the last few layers of the MobileNetV2 base model and training the classifier on top of it. Pay attention to not overfit!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpA8PlpQKygw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#acc += history_fine.history['accuracy']\n",
    "#val_acc += history_fine.history['val_accuracy']\n",
    "\n",
    "#loss += history_fine.history['loss']\n",
    "#val_loss += history_fine.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chW103JUItdk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(8, 8))\n",
    "#plt.subplot(2, 1, 1)\n",
    "#plt.plot(acc, label='Training Accuracy')\n",
    "#plt.plot(val_acc, label='Validation Accuracy')\n",
    "#plt.ylim([0, 1])\n",
    "#plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "#          plt.ylim(), label='Start Fine Tuning')\n",
    "#plt.legend(loc='lower right')\n",
    "#plt.title('Training and Validation Accuracy')\n",
    "\n",
    "#plt.subplot(2, 1, 2)\n",
    "#plt.plot(loss, label='Training Loss')\n",
    "#plt.plot(val_loss, label='Validation Loss')\n",
    "#plt.ylim([0, 1.0])\n",
    "#plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "#         plt.ylim(), label='Start Fine Tuning')\n",
    "#plt.legend(loc='upper right')\n",
    "#plt.title('Training and Validation Loss')\n",
    "#plt.xlabel('epoch')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6cWgjgfrsn5"
   },
   "source": [
    "### Evaluation and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSXH7PRMxOi5"
   },
   "source": [
    "Finaly you can verify the performance of the model on new data using test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2KyNhagHwfar",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Verify model performance on test set.\n",
    "\n",
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "print('Test accuracy :', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUNoQNgtfNgt",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve a batch of images from the test set\n",
    "image_batch, label_batch = test_dataset.as_numpy_iterator().next()\n",
    "predictions = model.predict_on_batch(image_batch).flatten()\n",
    "\n",
    "predictions = tf.where(predictions < 0.5, 0, 1)\n",
    "\n",
    "print('Predictions:\\n', predictions.numpy())\n",
    "print('Labels:\\n', label_batch)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "  ax = plt.subplot(3, 3, i + 1)\n",
    "  plt.imshow(image_batch[i].astype(\"uint8\"))\n",
    "  plt.title(class_names[predictions[i]])\n",
    "  plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with pictures obtained through web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create browser object and pass google images url to it.\n",
    "browser = mechanicalsoup.StatefulBrowser()\n",
    "url = 'https://www.google.ch/imghp?hl=en&ogbl'\n",
    "    \n",
    "browser.open(url)\n",
    "print(browser.get_url())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find pictures of cats...\n",
    "\n",
    "browser.get_current_page()\n",
    "\n",
    "browser.select_form()\n",
    "#browser.get_current_form().print_summary()\n",
    "\n",
    "search_term = 'Katze'\n",
    "browser['q'] = search_term\n",
    "\n",
    "browser.launch_browser()\n",
    "response = browser.submit_selected()\n",
    "\n",
    "new_url = browser.get_url()\n",
    "browser.open(new_url)\n",
    "\n",
    "page = browser.get_current_page()\n",
    "all_images = page.find_all('img')\n",
    "\n",
    "#... and save them into a list.\n",
    "\n",
    "image_source_cats = []\n",
    "for image in all_images:\n",
    "    image = image.get('src')\n",
    "    image_source_cats.append(image)\n",
    "    \n",
    "image_source_cats = [image for image in image_source_cats if image.startswith('https')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find pictures of dogs...\n",
    "\n",
    "browser.get_current_page()\n",
    "\n",
    "browser.select_form()\n",
    "#browser.get_current_form().print_summary()\n",
    "\n",
    "search_term = 'Hund'\n",
    "browser['q'] = search_term\n",
    "\n",
    "browser.launch_browser()\n",
    "response = browser.submit_selected()\n",
    "\n",
    "new_url = browser.get_url()\n",
    "browser.open(new_url)\n",
    "\n",
    "page = browser.get_current_page()\n",
    "all_images = page.find_all('img')\n",
    "\n",
    "#... and save them into a list.\n",
    "\n",
    "image_source_dogs = []\n",
    "for image in all_images:\n",
    "    image = image.get('src')\n",
    "    image_source_dogs.append(image)\n",
    "    \n",
    "image_source_dogs = [image for image in image_source_dogs if image.startswith('https')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "dst_dir_wbscrpng = os.path.join(path + '/catsordogs')\n",
    "os.makedirs(dst_dir_wbscrpng, exist_ok=True)\n",
    "\n",
    "#save cats\n",
    "\n",
    "c = 0\n",
    "\n",
    "for image in image_source_cats:\n",
    "    save_as = os.path.join(dst_dir_wbscrpng, 'catordog' + str(c) + '.jpg')\n",
    "    wget.download(image, save_as)\n",
    "    c += 1\n",
    "\n",
    "#save dogs   \n",
    "\n",
    "d = c\n",
    "\n",
    "for image in image_source_dogs:\n",
    "    save_as = os.path.join(dst_dir_wbscrpng, 'catordog' + str(d) + '.jpg')\n",
    "    wget.download(image, save_as)\n",
    "    d += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make and print predictions\n",
    "\n",
    "images_to_predict = os.listdir('./catsordogs')\n",
    "\n",
    "for i in images_to_predict: \n",
    "    img = tf.keras.utils.load_img(dst_dir_wbscrpng + \"/\" +i, target_size=(192,192))\n",
    "\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0) # Add the image to a batch where it's the only member.\n",
    "\n",
    "    prediction = model.predict_on_batch(img_array).flatten() #Predict on batch fügt eine Dimension hinzu, welche durch flatten wieder gelöscht werden soll.\n",
    "\n",
    "    prediction = tf.where(prediction < 0.5, 0, 1) #das ist die threshold. 50% = 0, 50% = 1.\n",
    "    \n",
    "    print(\n",
    "        \"This image most likely belongs to {}.\"\n",
    "        .format(class_names[prediction[0]])\n",
    "        )\n",
    "    #making sure every image is printed with it's prediction.\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    time.sleep(0.01)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transfer_learning.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
