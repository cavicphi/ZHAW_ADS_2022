{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRTa3Ee15WsJ"
   },
   "source": [
    "# ADS Project 2022 Group 7:  Image Classification with Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ONLY RELEVANT FOR AWS SAGEMAKER: installing mechanicalsoup\n",
    "!pip install mechanicalsoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ONLY RELEVANT FOR AWS SAGEMAKER: installing wget\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ONLY RELEVANT FOR AWS SAGEMAKER: installing tensorflow\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY RELEVANT FOR AWS SAGEMAKER: installing kaggle\n",
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stuff for web scraping\n",
    "import mechanicalsoup\n",
    "import wget\n",
    "\n",
    "#stuff to handle files and folders\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "#other stuff\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY RELEVANT FOR AWS SAGEMAKER: Kaggle secret\n",
    "#Step 1: Upload kaggle secret to jupyter notebook, to the same folder as this notebook is in. \n",
    "#Step 2: Uncomment and run the line of code below to move kaggle secret to the right location.\n",
    "#mv /home/ec2-user/SageMaker/kaggle.json /home/ec2-user/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to kaggle api\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download dataset from kaggle using api\n",
    "api.dataset_download_files('shaunthesheep/microsoft-catsvsdogs-dataset', path='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzip the zip file\n",
    "import zipfile\n",
    "with zipfile.ZipFile('microsoft-catsvsdogs-dataset.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('catsvsdogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize a subset of images of dogs\n",
    "dst_dir = 'catsvsdogs/resized/dogs'\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "files = glob.glob('./catsvsdogs/PetImages/Dog/*.jpg')\n",
    "\n",
    "i = 0\n",
    "\n",
    "for f in files: \n",
    "    i = i + 1\n",
    "    try:\n",
    "        img = Image.open(f)\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        img_resize = img.resize((192, 192))\n",
    "        root, ext = os.path.splitext(f)\n",
    "        basename = os.path.basename(root)\n",
    "        img_resize.save(os.path.join(dst_dir, basename + ext))\n",
    "    except (IOError) as e: #in some instances there were some corrupted files in the zip file.\n",
    "        print ('Bad file:', f)\n",
    "        pass\n",
    "    if i == 3000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resize a subset of images of cats \n",
    "dst_dir = 'catsvsdogs/resized/cats'\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "files = glob.glob('./catsvsdogs/PetImages/Cat/*.jpg')\n",
    "\n",
    "i = 0\n",
    "\n",
    "for f in files:\n",
    "    i = i + 1\n",
    "    try:\n",
    "        img = Image.open(f)\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        img_resize = img.resize((192, 192))\n",
    "        root, ext = os.path.splitext(f)\n",
    "        basename = os.path.basename(root)\n",
    "        img_resize.save(os.path.join(dst_dir, basename + ext))\n",
    "    except (IOError) as e: #in some instances there were some corrupted files in the zip file.\n",
    "        print ('Bad file:', f)\n",
    "        pass\n",
    "    if i == 3000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making sure the folders were created correctly\n",
    "data_dir = os.path.join(os.curdir, 'catsvsdogs/resized')\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONAL: Delete original unziped data\n",
    "\n",
    "shutil.rmtree(os.path.join(os.curdir, 'catsvsdogs/PetImages'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GoKGm1duzgk"
   },
   "source": [
    "## Image Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ro4oYaEmxe4r",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (192, 192)\n",
    "\n",
    "#shuffle immer mit seed, sonst kann man nicht das gleiche Dataset in Valdiation & Train verwenden.\n",
    "\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(data_dir,\n",
    "                                                            batch_size=BATCH_SIZE,\n",
    "                                                            image_size=IMG_SIZE,\n",
    "                                                            shuffle=True,\n",
    "                                                            seed=999,\n",
    "                                                            validation_split=0.20,\n",
    "                                                            subset = \"training\")\n",
    "\n",
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(data_dir,\n",
    "                                                            batch_size=BATCH_SIZE,\n",
    "                                                            image_size=IMG_SIZE,\n",
    "                                                            shuffle=True,\n",
    "                                                            seed=999,\n",
    "                                                            validation_split=0.20,\n",
    "                                                            subset = \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5BeQyKThC_Y",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_names = train_dataset.class_names\n",
    "\n",
    "#plt.figure(figsize=(10, 10))\n",
    "#for images, labels in train_dataset.take(1):\n",
    "#  for i in range(9):\n",
    "#    ax = plt.subplot(3, 3, i + 1)\n",
    "#    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "#    plt.title(class_names[labels[i]])\n",
    "#    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZqCX_mpV3Mx"
   },
   "source": [
    "### Creation of test set by splitting the validation set into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFFIYrTFV9RO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_batches = tf.data.experimental.cardinality(validation_dataset)\n",
    "test_dataset = validation_dataset.take(val_batches // 10)\n",
    "validation_dataset = validation_dataset.skip(val_batches // 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q9pFlFWgBKgH",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\n",
    "print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3UUPdm86LNC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Buffered prefetching\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYfcVwYLiR98"
   },
   "source": [
    "### Use data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3P99QiMGit1A",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#To reduce overfitting, pictures are flipped and rotated\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "      tf.keras.layers.RandomFlip('horizontal'),\n",
    "      tf.keras.layers.RandomRotation(0.2),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQullOUHkm67",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test effect of augmentation\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    for image, _ in train_dataset.take(1):\n",
    "      plt.figure(figsize=(10, 10))\n",
    "      first_image = image[0]\n",
    "      for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
    "        plt.imshow(augmented_image[0] / 255)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAywKtuVn8uK"
   },
   "source": [
    "### Choose Transfer Learning Model \n",
    "\n",
    "The following pre-trained model is used as base model for this image classifier project: **MobileNetV2** developed by Google (dowloaded via  `tf.keras.applications.MobileNetV2` ). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cO0HM9JAQUFq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2NyJn4KQMux",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#rescale = tf.keras.layers.Rescaling(1./127.5, offset=-1)\n",
    "\n",
    "#OPEN QUESTION\n",
    "\n",
    "#Wäre das noch nötig?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkH-kazQecHB"
   },
   "source": [
    "We saw in the lesson that, wen doing trasnfer learning, we don't use all the architecture but only the feature extraction.\n",
    "The very last classification layer (on \"top\", as most diagrams of machine learning models go from bottom to top) is not very useful. Instead, you will follow the common practice to depend on the very last layer before the flatten operation. This layer is called the \"bottleneck layer\". The bottleneck layer features retain more generality as compared to the final/top layer.\n",
    "\n",
    "First, instantiate a MobileNet V2 model pre-loaded with weights trained on ImageNet. By specifying the **include_top=False** argument, you load a network that doesn't include the classification layers at the top, which is ideal for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19IQ2gqneqmS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqcsxoJIEVXZ"
   },
   "source": [
    "This feature extractor converts each `192x192x3` image into a `6x6x1280` block of features. Let's see what it does to an example batch of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-2LJL0EEUcx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#image_batch, label_batch = next(iter(train_dataset))\n",
    "#feature_batch = base_model(image_batch)\n",
    "#print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlx56nQtfe8Y"
   },
   "source": [
    "### a. Freeze weights, use the feature extraction as it is, attach a new classification head\n",
    "In this step, you will freeze the convolutional base created from the previous step and to use as a feature extractor. Additionally, you add a classifier on top of it and train the top-level classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fL6upiN3ekS"
   },
   "source": [
    "It is important to freeze the convolutional base before you compile and train the model. Freezing (by setting layer.trainable = False) prevents the weights in a given layer from being updated during training. MobileNet V2 has many layers, so setting the entire model's `trainable` flag to False will freeze all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTCJH4bphOeo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#freeze convolutional base before compiling and training the model.\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KpbzSmPkDa-N",
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now combine this model with a classification head, meaning some fully connected layers (tf.keras.layers.Dense). \n",
    "\n",
    "Build a model by chaining together the data augmentation, preproccesing, `base_model` and classification layers (note that the syntax is slighly different, we are using the [Keras Functional API]).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1p0OJBR6dOT"
   },
   "source": [
    "Apply a `tf.keras.layers.Dense` layer to convert these features into a single prediction per image. You don't need an activation function here because this prediction will be treated as a `logit`, or a raw prediction value. Positive numbers predict class 1, negative numbers predict class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DgzQX6Veb2WT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False) #We need to set `training=False` as our model contains a `BatchNormalization` layer. More explanation here https://www.tensorflow.org/guide/keras/transfer_learning\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x) #this layer has the same purpose as tf.keras.layers.Flatten(). Need to connect something 2D to something 1D. Ask if you want to know more :)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x) \n",
    "model = tf.keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0ylJXE_kRLi"
   },
   "source": [
    "Compile the model before training it. Since there are two classes, use the `tf.keras.losses.BinaryCrossentropy` loss with `from_logits=True` since the model provides a linear output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpR8HdyMhukJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_learning_rate = 0.1\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I8ARiyMFsgbH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxOcmVr0ydFZ"
   },
   "source": [
    "Above you see how many parameters you are actually training, and how many are frozen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxvgOYTDSWTx"
   },
   "source": [
    "### Train the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Om4O3EESkab1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_epochs = 10\n",
    "\n",
    "loss0, accuracy0 = model.evaluate(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cYT1c48CuSd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"initial loss: {:.2f}\".format(loss0))\n",
    "print(\"initial accuracy: {:.2f}\".format(accuracy0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsaRFlZ9B6WK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53OTCh3jnbwV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plot learning curves of training and validation accuracy / loss.\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foWMyyUHbc1j"
   },
   "source": [
    "Note: If you are wondering why the validation metrics are better than the training metrics, the main factor is because layers like `tf.keras.layers.BatchNormalization` and `tf.keras.layers.Dropout` affect accuracy during training. They are turned off when calculating validation loss.\n",
    "\n",
    "To a lesser extent, it is also because training metrics report the average for an epoch, while validation metrics are evaluated after the epoch, so validation metrics see a model that has trained slightly longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6cWgjgfrsn5"
   },
   "source": [
    "### Evaluation and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSXH7PRMxOi5"
   },
   "source": [
    "Finaly you can verify the performance of the model on new data using test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2KyNhagHwfar",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Verify model performance on test set.\n",
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "print('Test accuracy :', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUNoQNgtfNgt",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve a batch of images from the test set\n",
    "image_batch, label_batch = test_dataset.as_numpy_iterator().next()\n",
    "predictions = model.predict_on_batch(image_batch).flatten()\n",
    "\n",
    "predictions = tf.where(predictions < 0.5, 0, 1)\n",
    "\n",
    "print('Predictions:\\n', predictions.numpy())\n",
    "print('Labels:\\n', label_batch)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "  ax = plt.subplot(3, 3, i + 1)\n",
    "  plt.imshow(image_batch[i].astype(\"uint8\"))\n",
    "  plt.title(class_names[predictions[i]])\n",
    "  plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply model with pictures obtained through web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create browser object and pass google images url to it.\n",
    "browser = mechanicalsoup.StatefulBrowser()\n",
    "url = 'https://www.google.ch/imghp?hl=en&ogbl'\n",
    "    \n",
    "browser.open(url)\n",
    "print(browser.get_url())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find pictures of cats...\n",
    "browser.get_current_page()\n",
    "browser.select_form()\n",
    "#browser.get_current_form().print_summary()\n",
    "\n",
    "search_term = 'Katze'\n",
    "\n",
    "browser['q'] = search_term\n",
    "browser.launch_browser()\n",
    "response = browser.submit_selected()\n",
    "new_url = browser.get_url()\n",
    "browser.open(new_url)\n",
    "page = browser.get_current_page()\n",
    "all_images = page.find_all('img')\n",
    "\n",
    "#... and save them into a list.\n",
    "image_source_cats = []\n",
    "for image in all_images:\n",
    "    image = image.get('src')\n",
    "    image_source_cats.append(image)\n",
    "    \n",
    "image_source_cats = [image for image in image_source_cats if image.startswith('https')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find pictures of dogs...\n",
    "browser.get_current_page()\n",
    "browser.select_form()\n",
    "#browser.get_current_form().print_summary()\n",
    "\n",
    "search_term = 'Hund'\n",
    "\n",
    "browser['q'] = search_term\n",
    "browser.launch_browser()\n",
    "response = browser.submit_selected()\n",
    "new_url = browser.get_url()\n",
    "browser.open(new_url)\n",
    "page = browser.get_current_page()\n",
    "all_images = page.find_all('img')\n",
    "\n",
    "#... and save them into a list.\n",
    "image_source_dogs = []\n",
    "for image in all_images:\n",
    "    image = image.get('src')\n",
    "    image_source_dogs.append(image)\n",
    "    \n",
    "image_source_dogs = [image for image in image_source_dogs if image.startswith('https')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "dst_dir_wbscrpng = os.path.join(path + '/catsordogs')\n",
    "os.makedirs(dst_dir_wbscrpng, exist_ok=True)\n",
    "\n",
    "#save cats\n",
    "\n",
    "c = 0\n",
    "\n",
    "for image in image_source_cats:\n",
    "    save_as = os.path.join(dst_dir_wbscrpng, 'catordog' + str(c) + '.jpg')\n",
    "    wget.download(image, save_as)\n",
    "    c += 1\n",
    "\n",
    "#save dogs   \n",
    "\n",
    "d = c\n",
    "\n",
    "for image in image_source_dogs:\n",
    "    save_as = os.path.join(dst_dir_wbscrpng, 'catordog' + str(d) + '.jpg')\n",
    "    wget.download(image, save_as)\n",
    "    d += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make and print predictions\n",
    "\n",
    "images_to_predict = os.listdir('./catsordogs')\n",
    "\n",
    "for i in images_to_predict: \n",
    "    img = tf.keras.utils.load_img(dst_dir_wbscrpng + \"/\" +i, target_size=(192,192))\n",
    "\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0) # Add the image to a batch where it's the only member.\n",
    "\n",
    "    prediction = model.predict_on_batch(img_array).flatten() #Predict on batch fügt eine Dimension hinzu, welche durch flatten wieder gelöscht werden soll.\n",
    "\n",
    "    prediction = tf.where(prediction < 0.5, 0, 1) #das ist die threshold. 50% = 0, 50% = 1.\n",
    "    \n",
    "    print(\n",
    "        \"This image most likely belongs to {}:\"\n",
    "        .format(class_names[prediction[0]])\n",
    "        )\n",
    "    #making sure every image is printed with it's prediction.\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    time.sleep(0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transfer_learning.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
